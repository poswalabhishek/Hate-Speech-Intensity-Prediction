{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8091ef09-cfb2-4c69-845e-aa7909a80484",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9153d2b9-ee88-4b00-84ec-f9277da36792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 3s 5ms/step - loss: 6.2205 - mse: 6.2205 - val_loss: 3.1833 - val_mse: 3.1833\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 3.4536 - mse: 3.4536 - val_loss: 2.4769 - val_mse: 2.4769\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 2.8593 - mse: 2.8593 - val_loss: 2.2247 - val_mse: 2.2247\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 2.6979 - mse: 2.6979 - val_loss: 2.1576 - val_mse: 2.1576\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 2.4822 - mse: 2.4822 - val_loss: 2.0779 - val_mse: 2.0779\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 2.3544 - mse: 2.3544 - val_loss: 2.0304 - val_mse: 2.0304\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 2.1764 - mse: 2.1764 - val_loss: 2.0349 - val_mse: 2.0349\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 2.1481 - mse: 2.1481 - val_loss: 2.2354 - val_mse: 2.2354\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 2.0787 - mse: 2.0787 - val_loss: 1.9699 - val_mse: 1.9699\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 2.0051 - mse: 2.0051 - val_loss: 2.0019 - val_mse: 2.0019\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.2745 - mse: 2.2745\n",
      "Test MAE: 2.2745494842529297\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "    return embeddings\n",
    "\n",
    "# Create embedding matrix\n",
    "def create_embedding_matrix(embeddings, tokenizer, embedding_dim=100):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 128\n",
    "embedding_dim = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = '../glove/glove.6B.100d.txt'  # Update the path to your GloVe file\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "embedding_matrix = create_embedding_matrix(glove_embeddings, tokenizer, embedding_dim)\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                             output_dim=embedding_dim,\n",
    "                             weights=[embedding_matrix],\n",
    "                             trainable=False)(input_layer)\n",
    "\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.5)(pooling_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, np.array(train_intensity), epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(test_padded, np.array(test_intensity))\n",
    "print(f'Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0993eeb0-29bb-43df-b41c-feef39ae6316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step\n",
      "Mean Squared Error: 2.274549398407914\n",
      "Pearson Correlation Coefficient: 0.6992633994535645\n",
      "Cosine Similarity: 0.9683171795946206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "# Ensure predictions are a NumPy array\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a66b1b-98bc-4e48-a4cc-ce2b8adfcf9e",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbba5f54-6eb5-404a-b655-b334a9b014f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 7s 26ms/step - loss: 5.8714 - mse: 5.8714 - val_loss: 4.1225 - val_mse: 4.1225\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 4.0810 - mse: 4.0810 - val_loss: 3.0697 - val_mse: 3.0697\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 3.0271 - mse: 3.0271 - val_loss: 2.4400 - val_mse: 2.4400\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 3s 23ms/step - loss: 2.4593 - mse: 2.4593 - val_loss: 2.4556 - val_mse: 2.4556\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 2.3778 - mse: 2.3778 - val_loss: 2.0210 - val_mse: 2.0210\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 2.2458 - mse: 2.2458 - val_loss: 2.0444 - val_mse: 2.0444\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 2.1355 - mse: 2.1355 - val_loss: 1.9398 - val_mse: 1.9398\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 2.0435 - mse: 2.0435 - val_loss: 2.0515 - val_mse: 2.0515\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 1.9609 - mse: 1.9609 - val_loss: 1.8302 - val_mse: 1.8302\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 3s 21ms/step - loss: 1.8542 - mse: 1.8542 - val_loss: 1.8041 - val_mse: 1.8041\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 2.1050 - mse: 2.1050\n",
      "Test MAE: 2.104975461959839\n",
      "38/38 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Bidirectional, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "    return embeddings\n",
    "\n",
    "# Create embedding matrix\n",
    "def create_embedding_matrix(embeddings, tokenizer, embedding_dim=100):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 128\n",
    "embedding_dim = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = '../glove/glove.6B.100d.txt'  # Update the path to your GloVe file\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "embedding_matrix = create_embedding_matrix(glove_embeddings, tokenizer, embedding_dim)\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                             output_dim=embedding_dim,\n",
    "                             weights=[embedding_matrix],\n",
    "                             trainable=False)(input_layer)\n",
    "\n",
    "# Bi-directional LSTM layer\n",
    "bilstm_layer = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\n",
    "dropout_layer = Dropout(0.5)(bilstm_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, np.array(train_intensity), epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(test_padded, np.array(test_intensity))\n",
    "print(f'Test MSE: {mse}')\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6facf2ca-39f7-4fbc-8ea7-7d6494d70699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.1049753536589706\n",
      "Pearson Correlation Coefficient: 0.7213071418065663\n",
      "Cosine Similarity: 0.9703194180768979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure predictions are a NumPy array\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123da6c-b21a-4477-a049-463ce9092a93",
   "metadata": {},
   "source": [
    "## BiLSTM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e700fcd0-3589-4d40-ac4e-30a57a2e03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 6s 18ms/step - loss: 4.7002 - mse: 4.7002 - val_loss: 2.6350 - val_mse: 2.6350\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 2.6953 - mse: 2.6953 - val_loss: 2.1155 - val_mse: 2.1155\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 2.3044 - mse: 2.3044 - val_loss: 1.9894 - val_mse: 1.9894\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 1s 10ms/step - loss: 2.0274 - mse: 2.0274 - val_loss: 1.9572 - val_mse: 1.9572\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 1s 10ms/step - loss: 1.9332 - mse: 1.9332 - val_loss: 1.9034 - val_mse: 1.9034\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 1.8277 - mse: 1.8277 - val_loss: 1.9300 - val_mse: 1.9300\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 1.6683 - mse: 1.6683 - val_loss: 1.8553 - val_mse: 1.8553\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 1.5703 - mse: 1.5703 - val_loss: 1.9040 - val_mse: 1.9040\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 1.5138 - mse: 1.5138 - val_loss: 1.8325 - val_mse: 1.8325\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 1s 11ms/step - loss: 1.4590 - mse: 1.4590 - val_loss: 1.9388 - val_mse: 1.9388\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 2.0995 - mse: 2.0995\n",
      "Test MAE: 2.0995113849639893\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "    return embeddings\n",
    "\n",
    "# Create embedding matrix\n",
    "def create_embedding_matrix(embeddings, tokenizer, embedding_dim=100):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 128\n",
    "embedding_dim = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = '../glove/glove.6B.100d.txt'  # Update the path to your GloVe file\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "embedding_matrix = create_embedding_matrix(glove_embeddings, tokenizer, embedding_dim)\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                             output_dim=embedding_dim,\n",
    "                             weights=[embedding_matrix],\n",
    "                             trainable=False)(input_layer)\n",
    "\n",
    "# Convolutional layer\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "\n",
    "# Bi-directional LSTM layer\n",
    "bilstm_layer = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\n",
    "\n",
    "# Concatenate CNN and BiLSTM outputs\n",
    "combined = tf.keras.layers.concatenate([pooling_layer, bilstm_layer])\n",
    "dropout_layer = Dropout(0.5)(combined)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, np.array(train_intensity), epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(test_padded, np.array(test_intensity))\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a80e1a-fa3d-48ad-b7ed-8335e432c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 7ms/step\n",
      "Mean Squared Error: 2.099511431687822\n",
      "Pearson Correlation Coefficient: 0.7227299781401003\n",
      "Cosine Similarity: 0.9705215784110299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure predictions are a NumPy array\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac3e5b-bd29-4568-a105-838d4a2382b4",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16acfb7-f435-4c56-b70b-e4fd9c026dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 3s 19ms/step - loss: 5.1910 - mse: 5.1910 - val_loss: 1.6556 - val_mse: 1.6556\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 2.1947 - mse: 2.1947 - val_loss: 1.6193 - val_mse: 1.6193\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.7230 - mse: 1.7230 - val_loss: 1.6243 - val_mse: 1.6243\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.5487 - mse: 1.5487 - val_loss: 1.7088 - val_mse: 1.7088\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.3491 - mse: 1.3491 - val_loss: 1.7723 - val_mse: 1.7723\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.2873 - mse: 1.2873 - val_loss: 1.7947 - val_mse: 1.7947\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.1860 - mse: 1.1860 - val_loss: 1.7803 - val_mse: 1.7803\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.1361 - mse: 1.1361 - val_loss: 1.6933 - val_mse: 1.6933\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.1024 - mse: 1.1024 - val_loss: 1.8373 - val_mse: 1.8373\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 2s 18ms/step - loss: 1.0728 - mse: 1.0728 - val_loss: 1.8275 - val_mse: 1.8275\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.9264 - mse: 1.9264\n",
      "Test MSE: 1.9263602495193481\n",
      "38/38 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 1.9263600466285273\n",
      "Pearson Correlation Coefficient: 0.7517681438471154\n",
      "Cosine Similarity: 0.9731643350323889\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Load BERT tokenizer\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "# Tokenization and padding\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_and_pad(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors='tf', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "train_input_ids, _ = tokenize_and_pad(train_sentences)\n",
    "test_input_ids, _ = tokenize_and_pad(test_sentences)\n",
    "\n",
    "# Build the model\n",
    "vocab_size = tokenizer.vocab_size  # Use the BERT tokenizer vocabulary size\n",
    "embedding_dim = 768  # Typically 768 for BERT embeddings\n",
    "\n",
    "# Input layer\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "# Embedding layer (using random weights, can be initialized with pretrained embeddings if desired)\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input_ids)\n",
    "\n",
    "# Convolutional layer\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "\n",
    "# Pooling layer\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "\n",
    "# Dropout and output layer\n",
    "dropout_layer = Dropout(0.5)(pooling_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=train_input_ids,\n",
    "    y=np.array(train_intensity),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(\n",
    "    x=test_input_ids,\n",
    "    y=np.array(test_intensity)\n",
    ")\n",
    "print(f'Test MSE: {mse}')\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_input_ids)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21836b6c-f00f-44a4-884d-578e4fff5b61",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72d83d1-3b07-4c0c-b732-f429ac7d3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 7s 32ms/step - loss: 4.8977 - mae: 1.7374 - val_loss: 1.9717 - val_mae: 1.0920\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 1.9129 - mae: 1.0794 - val_loss: 1.6201 - val_mae: 0.9815\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 1.3516 - mae: 0.9139 - val_loss: 1.5983 - val_mae: 0.9699\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 1.1038 - mae: 0.8209 - val_loss: 1.6158 - val_mae: 0.9706\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.9382 - mae: 0.7505 - val_loss: 1.6555 - val_mae: 0.9625\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.7936 - mae: 0.6943 - val_loss: 1.7028 - val_mae: 0.9864\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.6999 - mae: 0.6552 - val_loss: 1.7291 - val_mae: 0.9795\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.5985 - mae: 0.6020 - val_loss: 1.7508 - val_mae: 0.9834\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.5802 - mae: 0.5883 - val_loss: 1.7411 - val_mae: 0.9864\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.5050 - mae: 0.5499 - val_loss: 1.6891 - val_mae: 0.9617\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8677 - mae: 1.0151\n",
      "Test MAE: 1.0151128768920898\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "Mean Squared Error: 1.8676682983354544\n",
      "Pearson Correlation Coefficient: 0.7659235157145619\n",
      "Cosine Similarity: 0.9737162920060269\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, Embedding, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Load BERT tokenizer\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "# Tokenization and padding\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_and_pad(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors='tf', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "train_input_ids, _ = tokenize_and_pad(train_sentences)\n",
    "test_input_ids, _ = tokenize_and_pad(test_sentences)\n",
    "\n",
    "# Build the model\n",
    "vocab_size = tokenizer.vocab_size  # Use the BERT tokenizer vocabulary size\n",
    "embedding_dim = 768  # Typically 768 for BERT embeddings\n",
    "\n",
    "# Input layer\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "# Embedding layer (using random weights, can be initialized with pretrained embeddings if desired)\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input_ids)\n",
    "\n",
    "# BiLSTM layer\n",
    "bilstm_output = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "\n",
    "# Pooling layer\n",
    "pooling_layer = GlobalMaxPooling1D()(bilstm_output)\n",
    "\n",
    "# Dropout and output layer\n",
    "dropout_layer = Dropout(0.5)(pooling_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=train_input_ids,\n",
    "    y=np.array(train_intensity),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(\n",
    "    x=test_input_ids,\n",
    "    y=np.array(test_intensity)\n",
    ")\n",
    "print(f'Test MSE: {mse}')\n",
    "\n",
    "predictions = model.predict(test_input_ids)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dccaec-3dfc-4901-a947-16cb67a9f612",
   "metadata": {},
   "source": [
    "## BiLSTM + CNN (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0f21e0-1e6c-4d57-a698-e185a1886cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 7s 32ms/step - loss: 4.5200 - mse: 4.5200 - val_loss: 1.7560 - val_mse: 1.7560\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 1.6039 - mse: 1.6039 - val_loss: 1.6779 - val_mse: 1.6779\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 1.1285 - mse: 1.1285 - val_loss: 1.6911 - val_mse: 1.6911\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.8743 - mse: 0.8743 - val_loss: 1.7718 - val_mse: 1.7718\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.7215 - mse: 0.7215 - val_loss: 1.7769 - val_mse: 1.7769\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.5984 - mse: 0.5984 - val_loss: 1.7899 - val_mse: 1.7899\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.5537 - mse: 0.5537 - val_loss: 1.7618 - val_mse: 1.7618\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.4910 - mse: 0.4910 - val_loss: 1.8975 - val_mse: 1.8975\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 4s 26ms/step - loss: 0.4643 - mse: 0.4643 - val_loss: 1.8136 - val_mse: 1.8136\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 4s 27ms/step - loss: 0.4144 - mse: 0.4144 - val_loss: 1.7520 - val_mse: 1.7520\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.8006 - mse: 1.8006\n",
      "Test MSE: 1.800592303276062\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "Mean Squared Error: 1.8005924766931944\n",
      "Pearson Correlation Coefficient: 0.7698896371740548\n",
      "Cosine Similarity: 0.9747284936749111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Load BERT tokenizer\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "# Tokenization and padding\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_and_pad(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors='tf', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "train_input_ids, _ = tokenize_and_pad(train_sentences)\n",
    "test_input_ids, _ = tokenize_and_pad(test_sentences)\n",
    "\n",
    "# Build the model\n",
    "vocab_size = tokenizer.vocab_size  # Use the BERT tokenizer vocabulary size\n",
    "embedding_dim = 768  # Typically 768 for BERT embeddings\n",
    "\n",
    "# Input layer\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "# Embedding layer (using random weights, can be initialized with pretrained embeddings if desired)\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(input_ids)\n",
    "\n",
    "# Convolutional layer\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "\n",
    "# BiLSTM layer\n",
    "bilstm_output = Bidirectional(LSTM(128, return_sequences=True))(conv_layer)\n",
    "\n",
    "# Pooling layer\n",
    "pooling_layer = GlobalMaxPooling1D()(bilstm_output)\n",
    "\n",
    "# Dropout and output layer\n",
    "dropout_layer = Dropout(0.5)(pooling_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=train_input_ids,\n",
    "    y=np.array(train_intensity),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(\n",
    "    x=test_input_ids,\n",
    "    y=np.array(test_intensity)\n",
    ")\n",
    "print(f'Test MSE: {mse}')\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(test_input_ids)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c0f3f5-a7ae-44ee-b5a6-f974ac93f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "273/273 [==============================] - 125s 398ms/step - loss: 7.2070 - mse: 7.2070 - val_loss: 4.2276 - val_mse: 4.2276\n",
      "Epoch 2/10\n",
      "273/273 [==============================] - 106s 389ms/step - loss: 5.5628 - mse: 5.5628 - val_loss: 4.2626 - val_mse: 4.2626\n",
      "Epoch 3/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 5.0898 - mse: 5.0898 - val_loss: 4.3262 - val_mse: 4.3262\n",
      "Epoch 4/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.8194 - mse: 4.8194 - val_loss: 4.2493 - val_mse: 4.2493\n",
      "Epoch 5/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.7468 - mse: 4.7468 - val_loss: 4.2821 - val_mse: 4.2821\n",
      "Epoch 6/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.7718 - mse: 4.7718 - val_loss: 4.6221 - val_mse: 4.6221\n",
      "Epoch 7/10\n",
      "273/273 [==============================] - 107s 390ms/step - loss: 4.6844 - mse: 4.6844 - val_loss: 4.2295 - val_mse: 4.2295\n",
      "Epoch 8/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.5777 - mse: 4.5777 - val_loss: 4.4880 - val_mse: 4.4880\n",
      "Epoch 9/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.6083 - mse: 4.6083 - val_loss: 4.2347 - val_mse: 4.2347\n",
      "Epoch 10/10\n",
      "273/273 [==============================] - 106s 390ms/step - loss: 4.5872 - mse: 4.5872 - val_loss: 4.2317 - val_mse: 4.2317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2936161de50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"../dataset/i_train.csv\")\n",
    "train_sentences = train_data['sentence'].tolist()\n",
    "train_intensity = train_data['intensity'].tolist() \n",
    "\n",
    "test_data = pd.read_csv(\"../dataset/i_test.csv\")\n",
    "test_sentences = test_data['sentence'].tolist()\n",
    "test_intensity = test_data['intensity'].tolist() \n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert_model = TFBertModel.from_pretrained(BERT_MODEL)\n",
    "\n",
    "# Tokenization and padding\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_and_pad(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors='tf', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "train_input_ids, train_attention_masks = tokenize_and_pad(train_sentences)\n",
    "test_input_ids, test_attention_masks = tokenize_and_pad(test_sentences)\n",
    "\n",
    "# Build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_masks')\n",
    "\n",
    "# BERT layer\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_masks)[1]  # Use the pooled output\n",
    "\n",
    "# Dropout and output layer\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)(bert_output)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=[train_input_ids, train_attention_masks],\n",
    "    y=np.array(train_intensity),\n",
    "    epochs=10,  # Reduce epochs for quicker training; adjust as needed\n",
    "    batch_size=16,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d38f1ca-d5e7-4b32-80d8-8b8781aec2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 9s 242ms/step - loss: 4.4219 - mse: 4.4219\n",
      "Test MSE: 4.421870231628418\n",
      "38/38 [==============================] - 12s 239ms/step\n",
      "Mean Squared Error: 4.421870623404657\n",
      "Pearson Correlation Coefficient: 0.02498798820637999\n",
      "Cosine Similarity: 0.9372334711621242\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(\n",
    "    x=[test_input_ids, test_attention_masks],\n",
    "    y=np.array(test_intensity)\n",
    ")\n",
    "print(f'Test MSE: {mse}')\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "test_intensity = np.array(test_intensity).flatten()\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(test_intensity, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(test_intensity, predictions)\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(test_intensity.reshape(1, -1), predictions.reshape(1, -1))[0][0]\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7963fdf2-17eb-4e4f-8c49-582c08e84a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'bilm/CNN/Conv2D_2' defined at (most recent call last):\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_9024\\117836428.py\", line 33, in <module>\n      elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_hub\\module_v2.py\", line 126, in load\n      obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\nNode: 'bilm/CNN/Conv2D_2'\nOOM when allocating tensor with shape[4843,64,112,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bilm/CNN/Conv2D_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_pruned_4791]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m intensity_value, hate_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mc)\n\u001b[0;32m     54\u001b[0m X_tr, X_te, y_tr, y_te \u001b[38;5;241m=\u001b[39m train_test_split(hate_sentences, intensity_value, test_size\u001b[38;5;241m=\u001b[39mTEST_SIZE, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m train_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43melmo_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m elmo_embedding(X_te)\n\u001b[0;32m     59\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m train_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36melmo_embedding\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melmo_embedding\u001b[39m(sentences):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43melmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melmo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1604\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;124;03m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1604\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py:243\u001b[0m, in \u001b[0;36mWrappedFunction._call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m    241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_flat(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWrappedFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1622\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1620\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m structured_err\n\u001b[1;32m-> 1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_flat_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1671\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1666\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1667\u001b[0m       arg, (ops\u001b[38;5;241m.\u001b[39mTensor, resource_variable_ops\u001b[38;5;241m.\u001b[39mBaseResourceVariable)):\n\u001b[0;32m   1668\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_signature_summary()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1669\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(zero-based) to be a Tensor; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1670\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'bilm/CNN/Conv2D_2' defined at (most recent call last):\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_9024\\117836428.py\", line 33, in <module>\n      elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_hub\\module_v2.py\", line 126, in load\n      obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\nNode: 'bilm/CNN/Conv2D_2'\nOOM when allocating tensor with shape[4843,64,112,48] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bilm/CNN/Conv2D_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_pruned_4791]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "\n",
    "BASE_FOLDER = \"../dataset/\"\n",
    "INPUT_FILE = \"hate_norm_combined.pkl\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "LSTM_UNITS = 50\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.2\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "random_seed(SEED)\n",
    "\n",
    "# Load ELMo model\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "\n",
    "def elmo_embedding(sentences):\n",
    "    return elmo.signatures['default'](tf.constant(sentences))['elmo']\n",
    "\n",
    "with open(BASE_FOLDER + INPUT_FILE, 'rb') as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "intensity_value = []\n",
    "hate_sentences = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    intensity_value.append(int(input_data['Original_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Sentence'][i])\n",
    "    intensity_value.append(int(input_data['Normalized_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Normalized_Sentence'][i])\n",
    "\n",
    "c = list(zip(intensity_value, hate_sentences))\n",
    "random.shuffle(c)\n",
    "intensity_value, hate_sentences = zip(*c)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(hate_sentences, intensity_value, test_size=TEST_SIZE, random_state=1)\n",
    "\n",
    "train_embeddings = elmo_embedding(X_tr)\n",
    "test_embeddings = elmo_embedding(X_te)\n",
    "\n",
    "input_shape = train_embeddings.shape[1:]\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=input_shape, dtype=tf.float32, name='elmo_embeddings')\n",
    "bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT))(input_layer)\n",
    "att_layer = tf.keras.layers.Attention(use_scale=True)([bi_lstm, bi_lstm])\n",
    "global_max_pool = tf.keras.layers.GlobalMaxPool1D()(att_layer)\n",
    "dense_layer = tf.keras.layers.Dense(DENSE_UNITS, activation='relu')(global_max_pool)\n",
    "dropout_layer = tf.keras.layers.Dropout(DENSE_DROPOUT)(dense_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='linear')(dropout_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc', tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)\n",
    "\n",
    "model.fit(x=train_embeddings, y=y_tr, epochs=EPOCHS, validation_split=0.1, batch_size=BATCH_SIZE)\n",
    "\n",
    "results = model.evaluate(x=test_embeddings, y=y_te)\n",
    "print(results)\n",
    "predictions = model.predict(test_embeddings).flatten()\n",
    "\n",
    "mse = np.mean((predictions - y_te) ** 2)\n",
    "pearson_corr, _ = stats.pearsonr(predictions, y_te)\n",
    "cosine_sim = 1 - distance.cosine(predictions, y_te)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr}')\n",
    "print(f'Cosine Similarity: {cosine_sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5fd204-10fa-42a5-9bda-63ff2ad17cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_3 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_token[0][0]',            \n",
      "                                thPoolingAndCrossAt               'masked_token[0][0]']           \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=((N                                               \n",
      "                                one, 12, None, 128)                                               \n",
      "                                , (None, 12, None,                                                \n",
      "                                128),                                                             \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28)),                                                             \n",
      "                                 cross_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 128, 100)    327600      ['tf_bert_model_3[0][12]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_3 (Attention)        (None, 128, 100)     1           ['bidirectional_3[0][0]',        \n",
      "                                                                  'bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 100)         0           ['attention_3[0][0]']            \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50)           5050        ['global_max_pooling1d_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_151 (Dropout)          (None, 50)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            51          ['dropout_151[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,814,942\n",
      "Trainable params: 332,702\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4843/4843 [00:03<00:00, 1446.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1211/1211 [00:00<00:00, 1546.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 142s 905ms/step - loss: 0.0515 - acc: 0.0039 - root_mean_squared_error: 0.2270 - val_loss: 0.0474 - val_acc: 0.0062 - val_root_mean_squared_error: 0.2176\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 118s 862ms/step - loss: 0.0455 - acc: 0.0050 - root_mean_squared_error: 0.2133 - val_loss: 0.0408 - val_acc: 0.0041 - val_root_mean_squared_error: 0.2021\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 118s 863ms/step - loss: 0.0411 - acc: 0.0060 - root_mean_squared_error: 0.2026 - val_loss: 0.0373 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1931\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 118s 863ms/step - loss: 0.0376 - acc: 0.0053 - root_mean_squared_error: 0.1939 - val_loss: 0.0360 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1896\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 119s 866ms/step - loss: 0.0354 - acc: 0.0050 - root_mean_squared_error: 0.1881 - val_loss: 0.0311 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1763\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 116s 848ms/step - loss: 0.0325 - acc: 0.0053 - root_mean_squared_error: 0.1803 - val_loss: 0.0292 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1709\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 119s 873ms/step - loss: 0.0310 - acc: 0.0060 - root_mean_squared_error: 0.1761 - val_loss: 0.0296 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1720\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 119s 871ms/step - loss: 0.0301 - acc: 0.0057 - root_mean_squared_error: 0.1735 - val_loss: 0.0285 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1689\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 118s 863ms/step - loss: 0.0290 - acc: 0.0057 - root_mean_squared_error: 0.1704 - val_loss: 0.0283 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1683\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 118s 863ms/step - loss: 0.0281 - acc: 0.0062 - root_mean_squared_error: 0.1677 - val_loss: 0.0286 - val_acc: 0.0041 - val_root_mean_squared_error: 0.1692\n",
      "TEST split 0.2\n",
      "38/38 [==============================] - 12s 303ms/step - loss: 0.0311 - acc: 0.0050 - root_mean_squared_error: 0.1765\n",
      "[0.031141238287091255, 0.004954583011567593, 0.17646880447864532]\n",
      "38/38 [==============================] - 15s 301ms/step\n",
      "Pearson correlation: PearsonRResult(statistic=0.6707108436443342, pvalue=4.498327054140168e-159)\n",
      "Cosine similarity: 0.9655380382758498\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, BertConfig, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "\n",
    "BASE_FOLDER = \"../dataset/\"\n",
    "INPUT_FILE = \"hate_norm_combined.pkl\"\n",
    "BERT_MODEL = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "\n",
    "USE_ATT = True\n",
    "\n",
    "BERT_DROPOUT = 0.2\n",
    "LSTM_UNITS = 50\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.2\n",
    "EPOCHS = 10  #(Default 10)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "random_seed(SEED)\n",
    "\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=MAX_LENGTH,\n",
    "                                       padding='max_length',\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])\n",
    "\n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(\n",
    "        input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n",
    "\n",
    "\n",
    "## Define base bert configs\n",
    "config = BertConfig(dropout=BERT_DROPOUT,\n",
    "                    attention_dropout=BERT_DROPOUT,\n",
    "                    output_attentions=True)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFBertModel.from_pretrained(BERT_MODEL, config=config)\n",
    "for layer in transformer_model.layers[:3]:  ## We are freezing first 3 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Defining tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL,\n",
    "                                                do_lower_case=True,\n",
    "                                                add_special_tokens=True,\n",
    "                                                max_length=MAX_LENGTH,\n",
    "                                                padding='max_length')\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "                                     name='input_token',\n",
    "                                     dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "                                       name='masked_token',\n",
    "                                       dtype='int32')\n",
    "embedding_layer = transformer_model(input_ids_in,\n",
    "                                    attention_mask=input_masks_in)[0]\n",
    "X = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS,\n",
    "                         return_sequences=True,\n",
    "                         dropout=LSTM_DROPOUT,\n",
    "                         recurrent_dropout=LSTM_DROPOUT,\n",
    "                         kernel_initializer='normal'))(embedding_layer)\n",
    "if USE_ATT:\n",
    "    X = tf.keras.layers.Attention(use_scale=True)([X, X])  # Use attention.\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(DENSE_UNITS,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='normal')(X)\n",
    "X = tf.keras.layers.Dropout(DENSE_DROPOUT)(X)\n",
    "X = tf.keras.layers.Dense(\n",
    "    1,\n",
    "    activation='sigmoid',  # Using sigmoid instead of linear here.\n",
    "    kernel_initializer='normal')(X)\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',  # Treat HIP as a regression problem\n",
    "    metrics=['acc', tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "with open(BASE_FOLDER + INPUT_FILE, 'rb') as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "intensity_value = []\n",
    "hate_sentences = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    intensity_value.append(int(input_data['Original_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Sentence'][i])\n",
    "    intensity_value.append(int(input_data['Normalized_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Normalized_Sentence'][i])\n",
    "\n",
    "c = list(zip(intensity_value, hate_sentences))\n",
    "random.shuffle(c)\n",
    "intensity_value, hate_sentences = zip(*c)\n",
    "\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(hate_sentences,\n",
    "                                          intensity_value,\n",
    "                                          test_size=TEST_SIZE,\n",
    "                                          random_state=1)\n",
    "\n",
    "y_tr = (y_tr - 1) / 9  # Scale from [1, 10] to [0, 1]\n",
    "y_te = (y_te - 1) / 9  # Scale from [1, 10] to [0, 1]\n",
    "\n",
    "train_input_ids, train_input_masks, train_input_segment = tokenize(\n",
    "    X_tr, tokenizer)\n",
    "test_input_ids, test_input_masks, test_input_segment = tokenize(\n",
    "    X_te, tokenizer)\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)\n",
    "\n",
    "model.fit(x=[train_input_ids, train_input_masks],\n",
    "          y=y_tr,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.1,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"TEST split\", TEST_SIZE)\n",
    "results = model.evaluate(x=[test_input_ids, test_input_masks], y=y_te)\n",
    "print(results)\n",
    "result = model.predict(x=[test_input_ids, test_input_masks])\n",
    "\n",
    "result = np.array(result, dtype=np.float64).flatten()\n",
    "\n",
    "# Scale back to [1, 10]\n",
    "scaled_result = result * 9 + 1\n",
    "y_te = y_te * 9 + 1\n",
    "\n",
    "print(\"Pearson correlation:\", stats.pearsonr(scaled_result, y_te))\n",
    "print(\"Cosine similarity:\", 1 - distance.cosine(scaled_result, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feae22f-bb8e-4b54-9b53-10129fc96e74",
   "metadata": {},
   "source": [
    "## Unscaled Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1df83a3-42cc-4dc7-b597-4324f1e2524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_token[0][0]',            \n",
      "                                thPoolingAndCrossAt               'masked_token[0][0]']           \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=((N                                               \n",
      "                                one, 12, None, 128)                                               \n",
      "                                , (None, 12, None,                                                \n",
      "                                128),                                                             \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28)),                                                             \n",
      "                                 cross_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 128, 100)    327600      ['tf_bert_model_4[0][12]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attention_4 (Attention)        (None, 128, 100)     1           ['bidirectional_4[0][0]',        \n",
      "                                                                  'bidirectional_4[0][0]']        \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 100)         0           ['attention_4[0][0]']            \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50)           5050        ['global_max_pooling1d_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_189 (Dropout)          (None, 50)           0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            51          ['dropout_189[0][0]']            \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,814,942\n",
      "Trainable params: 332,702\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4843/4843 [00:03<00:00, 1461.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1211/1211 [00:00<00:00, 1482.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 140s 893ms/step - loss: 4.1738 - acc: 0.0067 - root_mean_squared_error: 2.0430 - val_loss: 3.8097 - val_acc: 0.0062 - val_root_mean_squared_error: 1.9518\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 116s 850ms/step - loss: 3.5299 - acc: 0.0067 - root_mean_squared_error: 1.8788 - val_loss: 3.0116 - val_acc: 0.0062 - val_root_mean_squared_error: 1.7354\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 116s 849ms/step - loss: 3.2363 - acc: 0.0067 - root_mean_squared_error: 1.7990 - val_loss: 2.9352 - val_acc: 0.0062 - val_root_mean_squared_error: 1.7133\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 116s 850ms/step - loss: 3.0444 - acc: 0.0067 - root_mean_squared_error: 1.7448 - val_loss: 2.8487 - val_acc: 0.0062 - val_root_mean_squared_error: 1.6878\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 116s 850ms/step - loss: 2.9345 - acc: 0.0067 - root_mean_squared_error: 1.7130 - val_loss: 2.7930 - val_acc: 0.0062 - val_root_mean_squared_error: 1.6712\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 116s 849ms/step - loss: 2.8463 - acc: 0.0067 - root_mean_squared_error: 1.6871 - val_loss: 2.5076 - val_acc: 0.0062 - val_root_mean_squared_error: 1.5836\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 116s 849ms/step - loss: 2.5687 - acc: 0.0067 - root_mean_squared_error: 1.6027 - val_loss: 2.3980 - val_acc: 0.0062 - val_root_mean_squared_error: 1.5485\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 116s 850ms/step - loss: 2.3895 - acc: 0.0067 - root_mean_squared_error: 1.5458 - val_loss: 2.4248 - val_acc: 0.0062 - val_root_mean_squared_error: 1.5572\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 116s 849ms/step - loss: 2.3254 - acc: 0.0067 - root_mean_squared_error: 1.5249 - val_loss: 2.1847 - val_acc: 0.0062 - val_root_mean_squared_error: 1.4781\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 116s 851ms/step - loss: 2.0983 - acc: 0.0067 - root_mean_squared_error: 1.4485 - val_loss: 2.0464 - val_acc: 0.0062 - val_root_mean_squared_error: 1.4305\n",
      "TEST split 0.2\n",
      "38/38 [==============================] - 11s 299ms/step - loss: 2.2884 - acc: 0.0058 - root_mean_squared_error: 1.5127\n",
      "[2.28839111328125, 0.005780346691608429, 1.5127428770065308]\n",
      "38/38 [==============================] - 15s 298ms/step\n",
      "Pearson correlation: PearsonRResult(statistic=0.698103728362522, pvalue=1.2697512063633943e-177)\n",
      "Cosine similarity: 0.9683089233253874\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, BertConfig, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "\n",
    "BASE_FOLDER = \"../dataset/\"\n",
    "INPUT_FILE = \"hate_norm_combined.pkl\"\n",
    "BERT_MODEL = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "\n",
    "USE_ATT = True\n",
    "\n",
    "BERT_DROPOUT = 0.2\n",
    "LSTM_UNITS = 50\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.2\n",
    "EPOCHS = 10  #(Default 10)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "random_seed(SEED)\n",
    "\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=MAX_LENGTH,\n",
    "                                       padding='max_length',\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])\n",
    "\n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(\n",
    "        input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n",
    "\n",
    "\n",
    "## Define base bert configs\n",
    "config = BertConfig(dropout=BERT_DROPOUT,\n",
    "                    attention_dropout=BERT_DROPOUT,\n",
    "                    output_attentions=True)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFBertModel.from_pretrained(BERT_MODEL, config=config)\n",
    "for layer in transformer_model.layers[:3]:  ## We are freezing first 3 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Defining tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL,\n",
    "                                                do_lower_case=True,\n",
    "                                                add_special_tokens=True,\n",
    "                                                max_length=MAX_LENGTH,\n",
    "                                                padding='max_length')\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "                                     name='input_token',\n",
    "                                     dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "                                       name='masked_token',\n",
    "                                       dtype='int32')l\n",
    "embedding_layer = transformer_model(input_ids_in,\n",
    "                                    attention_mask=input_masks_in)[0]\n",
    "X = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS,\n",
    "                         return_sequences=True,\n",
    "                         dropout=LSTM_DROPOUT,\n",
    "                         recurrent_dropout=LSTM_DROPOUT,\n",
    "                         kernel_initializer='normal'))(embedding_layer)\n",
    "if USE_ATT:\n",
    "    X = tf.keras.layers.Attention(use_scale=True)([X, X])  # Use attention.\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(DENSE_UNITS,\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='normal')(X)\n",
    "X = tf.keras.layers.Dropout(DENSE_DROPOUT)(X)\n",
    "X = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='normal')(X)\n",
    "\n",
    "# Custom scaling layer\n",
    "def scale_output(x):\n",
    "    return x * 9 + 1\n",
    "\n",
    "scaled_output = tf.keras.layers.Lambda(scale_output)(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=scaled_output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',  # Treat HIP as a regression problem\n",
    "    metrics=['acc', tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "with open(BASE_FOLDER + INPUT_FILE, 'rb') as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "intensity_value = []\n",
    "hate_sentences = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    intensity_value.append(int(input_data['Original_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Sentence'][i])\n",
    "    intensity_value.append(int(input_data['Normalized_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Normalized_Sentence'][i])\n",
    "\n",
    "c = list(zip(intensity_value, hate_sentences))\n",
    "random.shuffle(c)\n",
    "intensity_value, hate_sentences = zip(*c)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(hate_sentences,\n",
    "                                          intensity_value,\n",
    "                                          test_size=TEST_SIZE,\n",
    "                                          random_state=1)\n",
    "\n",
    "train_input_ids, train_input_masks, train_input_segment = tokenize(X_tr, tokenizer)\n",
    "test_input_ids, test_input_masks, test_input_segment = tokenize(X_te, tokenizer)\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)\n",
    "\n",
    "model.fit(x=[train_input_ids, train_input_masks],\n",
    "          y=y_tr,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.1,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"TEST split\", TEST_SIZE)\n",
    "results = model.evaluate(x=[test_input_ids, test_input_masks], y=y_te)\n",
    "print(results)\n",
    "result = model.predict(x=[test_input_ids, test_input_masks])\n",
    "\n",
    "result = np.array(result, dtype=np.float64).flatten()\n",
    "\n",
    "print(\"Pearson correlation:\", stats.pearsonr(result, y_te))\n",
    "print(\"Cosine similarity:\", 1 - distance.cosine(result, y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e60fd9-b82f-484c-b4b5-e0c0b5b3656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
