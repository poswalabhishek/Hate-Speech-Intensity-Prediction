{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJzB1IpB7Y63",
    "outputId": "58ff8715-5e02-4646-f8cf-1515d3685742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m The given API key api key is invalid on www.comet.com, please check it against the dashboard. Your experiment will not be logged \n",
      "For more details, please refer to: https://www.comet.com/docs/v2/api-and-sdk/python-sdk/warnings-errors/\n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m The given API key api key is invalid on www.comet.com, please check it against the dashboard. Your experiment will not be logged \n",
      "For more details, please refer to: https://www.comet.com/docs/v2/api-and-sdk/python-sdk/warnings-errors/\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "experiment = Experiment(\n",
    "  api_key=\"api key\",\n",
    "  project_name=\"project-name\",\n",
    "  workspace=\"workspace-name\",\n",
    "  log_graph=True, # Can be True or False.\n",
    "  auto_metric_logging=True, # Can be True or False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2j80Arfd7nUM"
   },
   "outputs": [],
   "source": [
    "experiment.set_name(\"HIP_distilled_roberta_bilstm-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGYa_QnB7wX5",
    "outputId": "3d0b7dde-bbdc-4422-8ec1-2fa1e38c1c09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFRobertaModel, RobertaConfig, RobertaTokenizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "BERT_MODEL = \"roberta-base\"  # Specify the RoBERTa model\n",
    "MAX_LENGTH = 128\n",
    "TEST_SIZE = 0.15\n",
    "SEED = 42\n",
    "\n",
    "USE_ATT = True\n",
    "\n",
    "BERT_DROPOUT = 0.2\n",
    "LSTM_UNITS = 512\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.2\n",
    "EPOCHS = 10 #(Default 10)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks = [], []\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=MAX_LENGTH,\n",
    "                                       pad_to_max_length=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=False)  # Roberta doesn't use token_type_ids\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "\n",
    "    return (np.asarray(input_ids, dtype='int32'),\n",
    "            np.asarray(input_masks, dtype='int32'))\n",
    "\n",
    "    ## Define base Roberta configs\n",
    "# config = RobertaConfig(dropout=BERT_DROPOUT,\n",
    "#                        attention_dropout=BERT_DROPOUT,\n",
    "#                        output_attentions=True)\n",
    "\n",
    "config = RobertaConfig.from_pretrained(BERT_MODEL,\n",
    "                                       output_hidden_states=False,\n",
    "                                       attention_probs_dropout_prob =BERT_DROPOUT,\n",
    "                                       output_attentions = True)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFRobertaModel.from_pretrained(BERT_MODEL, config=config)\n",
    "\n",
    "# Freeze the pre-trained layers for fine-tuning\n",
    "for layer in transformer_model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Defining tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=MAX_LENGTH,\n",
    "                                             pad_to_max_length=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H3xHgU0T8drJ"
   },
   "outputs": [],
   "source": [
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S4Nyqd4FHt1y"
   },
   "outputs": [],
   "source": [
    "# Roberta BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cbu7AY5_9Uf6",
    "outputId": "98864a3c-e1ab-4041-be6a-3a4f6271af15"
   },
   "outputs": [],
   "source": [
    "# input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "#                                      name='input_token',\n",
    "#                                      dtype='int32')\n",
    "# input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n",
    "#                                        name='masked_token',\n",
    "#                                        dtype='int32')\n",
    "# embedding_layer = transformer_model(input_ids_in,\n",
    "#                                     attention_mask=input_masks_in)[0]\n",
    "# X = tf.keras.layers.Bidirectional(\n",
    "#     tf.keras.layers.LSTM(LSTM_UNITS,\n",
    "#                          return_sequences=True,\n",
    "#                          dropout=LSTM_DROPOUT,\n",
    "#                          recurrent_dropout=LSTM_DROPOUT,\n",
    "#                          kernel_initializer='normal'))(embedding_layer)\n",
    "# if USE_ATT:\n",
    "#     X = tf.keras.layers.Attention(use_scale=True)([X, X])  # Use attention.\n",
    "# X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "# X = tf.keras.layers.Dense(DENSE_UNITS,\n",
    "#                           activation='relu',\n",
    "#                           kernel_initializer='normal')(X)\n",
    "# X = tf.keras.layers.Dropout(DENSE_DROPOUT)(X)\n",
    "# X = tf.keras.layers.Dense(\n",
    "#     1,\n",
    "#     activation='linear',  # Can be with activation=\"sigmoid\" here.\n",
    "#     kernel_initializer='normal')(X)\n",
    "# model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='mean_squared_error',  # Treat HIP as a regression problem\n",
    "#     metrics=['acc', tf.keras.metrics.RootMeanSquaredError()])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lgy9wAPFNZxp"
   },
   "outputs": [],
   "source": [
    "# Roberta RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTII84RVNb2z",
    "outputId": "dca14c13-81e3-4ad2-9676-bc4142265c3e"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# # Create the model\n",
    "# input_ids_in = Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
    "# input_masks_in = Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')\n",
    "\n",
    "# embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "# # Bi-directional LSTM layer\n",
    "# X_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(embedding_layer)\n",
    "\n",
    "# # Convolutional layer\n",
    "# X_conv = Conv1D(filters=128, kernel_size=2, activation='relu')(X_lstm)  # Apply CNN on LSTM output\n",
    "\n",
    "# # Global Max Pooling layer\n",
    "# X_pool = GlobalMaxPooling1D()(X_conv)\n",
    "\n",
    "# # Dense layers\n",
    "# X = Dense(DENSE_UNITS, activation='relu')(X_pool)\n",
    "# X = Dropout(DENSE_DROPOUT)(X)\n",
    "# X = Dense(1, activation='linear')(X)\n",
    "\n",
    "# model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='mean_squared_error',\n",
    "#               metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Fe0zhDNrRpnR"
   },
   "outputs": [],
   "source": [
    "# Roberta RCNN with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OFzBCqGRrro",
    "outputId": "df99437c-3783-444d-d876-df35780f0b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_token[0][0]',            \n",
      " el)                            thPoolingAndCrossAt               'masked_token[0][0]']           \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=((N                                               \n",
      "                                one, 12, None, 128)                                               \n",
      "                                , (None, 12, None,                                                \n",
      "                                128),                                                             \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28),                                                              \n",
      "                                 (None, 12, None, 1                                               \n",
      "                                28)),                                                             \n",
      "                                 cross_attentions=N                                               \n",
      "                                one)                                                              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128, 1024)    5246976     ['tf_roberta_model[0][12]']      \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 127, 128)     262272      ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 127, 128)     1           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 127, 256)     0           ['conv1d[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['concatenate[0][0]']            \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           12850       ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            51          ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 130,167,782\n",
      "Trainable params: 5,522,150\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Attention, Concatenate\n",
    "# Inputs\n",
    "input_ids_in = Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
    "input_masks_in = Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')\n",
    "\n",
    "# Transformer Model for Embedding\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "# Bi-directional LSTM layer\n",
    "X_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(embedding_layer)\n",
    "\n",
    "# Convolutional layer\n",
    "X_conv = Conv1D(filters=128, kernel_size=2, activation='relu')(X_lstm)  # Apply CNN on LSTM output\n",
    "\n",
    "# Attention layer\n",
    "USE_ATT = True  # set to True or False\n",
    "if USE_ATT:\n",
    "    X_att = Attention(use_scale=True)([X_conv, X_conv])\n",
    "    # Combining original convolution outputs and attention outputs\n",
    "    X_combined = Concatenate(axis=-1)([X_conv, X_att])\n",
    "    X_pool = GlobalMaxPooling1D()(X_combined)\n",
    "else:\n",
    "    X_pool = GlobalMaxPooling1D()(X_conv)\n",
    "\n",
    "# Dense layers\n",
    "X = Dense(DENSE_UNITS, activation='relu')(X_pool)\n",
    "X = Dropout(DENSE_DROPOUT)(X)\n",
    "X = Dense(1, activation='linear')(X)  # regression output\n",
    "\n",
    "# Final model\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H-vYvdxw9csY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv(\"../datasets/hate_norm_with_span.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckb_-IYP9hBr",
    "outputId": "50b81ed2-6e29-4707-ab07-7438baea38d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5145 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5145/5145 [00:01<00:00, 4316.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 909/909 [00:00<00:00, 5476.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "intensity_value = []\n",
    "hate_sentences = []\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    intensity_value.append(int(input_data['Original_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Sentence'][i])\n",
    "    intensity_value.append(int(input_data['Normalized_Intensity'][i]))\n",
    "    hate_sentences.append(input_data['Normalized_Sentence'][i])\n",
    "\n",
    "c = list(zip(intensity_value, hate_sentences))\n",
    "random.shuffle(c)\n",
    "intensity_value, hate_sentences = zip(*c)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(hate_sentences,\n",
    "                                          intensity_value,\n",
    "                                          test_size=TEST_SIZE,\n",
    "                                          random_state=1)\n",
    "\n",
    "train_input_ids, train_input_masks = tokenize(\n",
    "    X_tr, tokenizer)\n",
    "test_input_ids, test_input_masks = tokenize(\n",
    "    X_te, tokenizer)\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XieF0X-a9jMp",
    "outputId": "0a21c8c7-767a-4acb-bc05-3cb7a3ceb83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/137 [==============================] - 58s 338ms/step - loss: 3.8499 - accuracy: 0.0066 - root_mean_squared_error: 1.9621 - val_loss: 2.1923 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.4806\n",
      "Epoch 2/10\n",
      "137/137 [==============================] - 44s 318ms/step - loss: 2.5088 - accuracy: 0.0066 - root_mean_squared_error: 1.5839 - val_loss: 1.7367 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.3178\n",
      "Epoch 3/10\n",
      "137/137 [==============================] - 44s 318ms/step - loss: 2.0584 - accuracy: 0.0066 - root_mean_squared_error: 1.4347 - val_loss: 1.7278 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.3144\n",
      "Epoch 4/10\n",
      "137/137 [==============================] - 43s 317ms/step - loss: 1.8973 - accuracy: 0.0066 - root_mean_squared_error: 1.3774 - val_loss: 1.4427 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.2011\n",
      "Epoch 5/10\n",
      "137/137 [==============================] - 44s 318ms/step - loss: 1.7240 - accuracy: 0.0066 - root_mean_squared_error: 1.3130 - val_loss: 1.4672 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.2113\n",
      "Epoch 6/10\n",
      "137/137 [==============================] - 44s 320ms/step - loss: 1.5877 - accuracy: 0.0066 - root_mean_squared_error: 1.2600 - val_loss: 1.3548 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.1640\n",
      "Epoch 7/10\n",
      "137/137 [==============================] - 43s 317ms/step - loss: 1.5412 - accuracy: 0.0066 - root_mean_squared_error: 1.2415 - val_loss: 1.3519 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.1627\n",
      "Epoch 8/10\n",
      "137/137 [==============================] - 43s 317ms/step - loss: 1.4134 - accuracy: 0.0066 - root_mean_squared_error: 1.1889 - val_loss: 1.3395 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.1574\n",
      "Epoch 9/10\n",
      "137/137 [==============================] - 44s 318ms/step - loss: 1.2761 - accuracy: 0.0066 - root_mean_squared_error: 1.1296 - val_loss: 1.3204 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.1491\n",
      "Epoch 10/10\n",
      "137/137 [==============================] - 43s 317ms/step - loss: 1.2427 - accuracy: 0.0066 - root_mean_squared_error: 1.1147 - val_loss: 1.2397 - val_accuracy: 0.0052 - val_root_mean_squared_error: 1.1134\n",
      "TEST split 0.15\n",
      "29/29 [==============================] - 7s 225ms/step - loss: 1.4654 - accuracy: 0.0066 - root_mean_squared_error: 1.2105\n",
      "[1.4653772115707397, 0.0066006602719426155, 1.2105276584625244]\n",
      "29/29 [==============================] - 9s 221ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m     10\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x\u001b[38;5;241m=\u001b[39m[test_input_ids, test_input_masks])\n\u001b[1;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m)\n\u001b[0;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpear\u001b[39m\u001b[38;5;124m\"\u001b[39m, stats\u001b[38;5;241m.\u001b[39mpearsonr(result, y_te))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "model.fit(x=[train_input_ids, train_input_masks],\n",
    "          y=y_tr,\n",
    "          epochs=10,\n",
    "          validation_split=0.15,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST split 0.15\n",
      "29/29 [==============================] - 7s 222ms/step - loss: 1.4654 - accuracy: 0.0066 - root_mean_squared_error: 1.2105\n",
      "[1.4653772115707397, 0.0066006602719426155, 1.2105276584625244]\n",
      "29/29 [==============================] - 6s 220ms/step\n",
      "pear PearsonRResult(statistic=0.8114186820853257, pvalue=9.198447365035483e-214)\n",
      "cosine 0.9802076570751925\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST split\", TEST_SIZE)\n",
    "results = model.evaluate(x=[test_input_ids, test_input_masks], y=y_te)\n",
    "print(results)\n",
    "result = model.predict(x=[test_input_ids, test_input_masks])\n",
    "result = np.array(result, dtype=np.float64)\n",
    "result = result.flatten()\n",
    "print(\"pear\", stats.pearsonr(result, y_te))\n",
    "print(\"cosine\", 1 - distance.cosine(result, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hI82kgf29oEE",
    "outputId": "b54f0d5e-560d-49e4-f0c4-af1cce7eadff"
   },
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "My4i8fAtAM4L"
   },
   "outputs": [],
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "Mb5YGtGjHAwA",
    "outputId": "c19ee803-fb5b-4465-a626-6a2d55b45679"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_model() got an unexpected keyword argument 'show_trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_rcnn_orig.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrankdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_nested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_layer_activations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_model() got an unexpected keyword argument 'show_trainable'"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model_rcnn_orig.png',\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv6mzm9CALJ9"
   },
   "outputs": [],
   "source": [
    "# Roberta BiLSTM CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4x9Wj_fCTHK"
   },
   "outputs": [],
   "source": [
    "experiment.set_name(\"HIP_roberta_rcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmd1_hWcAPyY",
    "outputId": "050bb27b-e9c1-42e4-ca4d-d3701286c295"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set some parameters\n",
    "MAX_LENGTH = 128\n",
    "LSTM_UNITS = 512\n",
    "DENSE_UNITS = 64\n",
    "DENSE_DROPOUT = 0.2\n",
    "\n",
    "# Load the BERT model\n",
    "#transformer_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create the model\n",
    "input_ids_in = Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
    "input_masks_in = Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')\n",
    "\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "# Bi-directional LSTM layer\n",
    "X_lstm = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(embedding_layer)\n",
    "\n",
    "# Apply a pooling layer to the LSTM outputs\n",
    "X_lstm = GlobalMaxPooling1D()(X_lstm)\n",
    "\n",
    "# Convolutional layer\n",
    "X_conv = Conv1D(filters=128, kernel_size=2, activation='relu')(embedding_layer)\n",
    "\n",
    "# Global Max Pooling layer\n",
    "X_pool = GlobalMaxPooling1D()(X_conv)\n",
    "\n",
    "# Concatenation of all layers\n",
    "X_concat = Concatenate(axis=1)([X_lstm, X_pool])\n",
    "\n",
    "# Dense layers\n",
    "X = Dense(DENSE_UNITS, activation='relu')(X_concat)\n",
    "X = Dropout(DENSE_DROPOUT)(X)\n",
    "X = Dense(1, activation='linear')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "ZlY_VQJRH_Su",
    "outputId": "74a178e6-e62c-4fb0-bc4f-52c30285eec5"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model_rcnn.png',\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDKWpVn7CelU",
    "outputId": "bd35fe8c-4ebc-43fc-dde2-8db82632636d"
   },
   "outputs": [],
   "source": [
    "model.fit(x=[train_input_ids, train_input_masks],\n",
    "          y=y_tr,\n",
    "          epochs=10,\n",
    "          validation_split=0.1,\n",
    "          batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"TEST split\", TEST_SIZE)\n",
    "results = model.evaluate(x=[test_input_ids, test_input_masks], y=y_te)\n",
    "print(results)\n",
    "result = model.predict(x=[test_input_ids, test_input_masks])\n",
    "result = np.array(result, dtype=np.float)\n",
    "result = result.flatten()\n",
    "print(\"pear\", stats.pearsonr(result, y_te))\n",
    "print(\"cosine\", 1 - distance.cosine(result, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffVdyim1ChOT",
    "outputId": "83d948d5-28ea-4ed7-8b8f-ec1db5009c07"
   },
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5JXZLdVKdfo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
