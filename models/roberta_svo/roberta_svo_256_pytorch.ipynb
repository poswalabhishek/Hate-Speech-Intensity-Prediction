{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed4781a-f158-47a8-80a4-d03fe1176e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "print(\"Num GPUs Available: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e13f10-85f3-4564-8b22-62a40ec97c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Profanity</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Islam looks like a cult more than like a relig...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We must send Islamics back to their native cou...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe is civilised, Muslims should not stay t...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If they love Sharia law so much, why do not th...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Islam=evil. Islam is invading us and trying to...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Intensity  Profanity  \\\n",
       "0  Islam looks like a cult more than like a relig...        9.0          1   \n",
       "1  We must send Islamics back to their native cou...        7.0          0   \n",
       "2  Europe is civilised, Muslims should not stay t...        7.0          0   \n",
       "3  If they love Sharia law so much, why do not th...        8.0          0   \n",
       "4  Islam=evil. Islam is invading us and trying to...        7.0          0   \n",
       "\n",
       "                                             Subject  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "2                     [1, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                Verb  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                              Object  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = \"../../datasets/\"\n",
    "input_file = \"hate_int_prof_SVO.tsv\"\n",
    "\n",
    "output_folder = \"output_weights/\"\n",
    "output_file = \"output_weights\"\n",
    "\n",
    "roberta_model = \"roberta-base\"\n",
    "max_length = 256\n",
    "TEST_SIZE = 0.2\n",
    "seed = 42\n",
    "\n",
    "use_attention = True\n",
    "\n",
    "roberta_dropout = 0.2\n",
    "lstm_units = 256\n",
    "dense_units = 50\n",
    "lstm_dropout = 0.1\n",
    "dense_dropout = 0.2\n",
    "epochs = 10  # (Default 10)\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "random_seed(seed)\n",
    "\n",
    "dataframe = pd.read_table(base_folder + input_file)\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58dfd47-3833-4408-9c22-98726a07d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define base Roberta configs\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    roberta_model,\n",
    "    output_hidden_states=False,\n",
    "    attention_probs_dropout_prob=roberta_dropout,\n",
    "    output_attentions=True\n",
    ")\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load the Roberta model with the specified config\n",
    "transformer_model = RobertaModel.from_pretrained(roberta_model, config=config)\n",
    "\n",
    "# Freeze the first 3 layers\n",
    "for param in transformer_model.encoder.layer[:3]:\n",
    "    param.requires_grad = False  # Freeze first 3 layers\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    roberta_model,\n",
    "    do_lower_case=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    padding='max_length'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aae29bd-cc16-4b1e-9d3e-42e91ab0bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaLSTMModel(nn.Module):\n",
    "    def __init__(self, transformer_model, lstm_units, dense_units, roberta_dropout, lstm_dropout, dense_dropout, use_attention):\n",
    "        super(RobertaLSTMModel, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.lstm = nn.LSTM(input_size=transformer_model.config.hidden_size, hidden_size=lstm_units, num_layers=1, batch_first=True, bidirectional=True, dropout=lstm_dropout)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=2*lstm_units, num_heads=1) if use_attention else None\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dense = nn.Linear(2*lstm_units, dense_units)\n",
    "        self.dropout = nn.Dropout(dense_dropout)\n",
    "        self.output = nn.Linear(dense_units, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = transformer_output.last_hidden_state\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        if self.attention:\n",
    "            x = x.permute(1, 0, 2)  # Change to (seq_len, batch_size, hidden_size)\n",
    "            x, _ = self.attention(x, x, x)\n",
    "            x = x.permute(1, 0, 2)  # Change back to (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, hidden_size, seq_len)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        x = torch.relu(self.dense(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2662e3-498d-4275-b5c4-c454fb3c1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = RobertaLSTMModel(transformer_model, lstm_units, dense_units, roberta_dropout, lstm_dropout, dense_dropout, use_attention).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17796a2-1a7b-4f09-afbb-bcbffbcaca06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m input_size \u001b[38;5;241m=\u001b[39m (max_length,)  \u001b[38;5;66;03m# Example for sequence length of 256\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Use the torchsummary to print the model summary\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39min_size)\u001b[38;5;241m.\u001b[39mtype(dtype) \u001b[38;5;28;01mfor\u001b[39;00m in_size \u001b[38;5;129;01min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(dtype) \u001b[38;5;28;01mfor\u001b[39;00m in_size \u001b[38;5;129;01min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mTypeError\u001b[0m: rand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\""
     ]
    }
   ],
   "source": [
    "class RobertaLSTMModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(RobertaLSTMModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "model = RobertaLSTMModel(transformer_model, lstm_units, dense_units, roberta_dropout, lstm_dropout, dense_dropout, use_attention).to(device)\n",
    "wrapped_model = RobertaLSTMModelWrapper(model).to(device)\n",
    "\n",
    "# Define the input size (batch_size, sequence_length)\n",
    "input_size = (max_length,)  # Example for sequence length of 256\n",
    "\n",
    "# Use the torchsummary to print the model summary\n",
    "summary(wrapped_model, [(input_size,), (input_size,)], device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fec107e-2d18-4895-b11c-6c11dcd38d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, subject_embeddings, verb_embeddings, object_embeddings, svo_embeddings, tokenizer, sentence_length):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    \n",
    "    for sentence, subj_emb, verb_emb, obj_emb, svo_emb in tqdm(list(zip(sentences, subject_embeddings, verb_embeddings, object_embeddings, svo_embeddings))):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=sentence_length,\n",
    "                                       padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])\n",
    "\n",
    "        input_ids[-1].extend(svo_emb.tolist())\n",
    "        input_masks[-1].extend([1] * len(svo_emb))\n",
    "        input_segments[-1].extend([1] * len(svo_emb))\n",
    "    \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60650cf5-1523-4329-9c0a-59c95ed582aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen, padding='post'):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < maxlen:\n",
    "            if padding == 'post':\n",
    "                padded_seq = F.pad(torch.tensor(seq), (0, maxlen - len(seq)), \"constant\", 0)\n",
    "            else:\n",
    "                padded_seq = F.pad(torch.tensor(seq), (maxlen - len(seq), 0), \"constant\", 0)\n",
    "        else:\n",
    "            padded_seq = torch.tensor(seq[:maxlen])\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcde2575-2ead-4692-8e2c-a4088a726135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4843/4843 [00:01<00:00, 3923.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1211/1211 [00:00<00:00, 5003.66it/s]\n"
     ]
    }
   ],
   "source": [
    "input_data = pd.read_table(base_folder + input_file)\n",
    "\n",
    "sentences = input_data['Sentence'].tolist()\n",
    "intensity_value = input_data['Intensity'].astype(int).tolist()\n",
    "\n",
    "SVO_length = 128\n",
    "subject_embeddings = pad_sequences([np.fromstring(embedding[1:-1], dtype=int, sep=',') for embedding in input_data[\"Subject\"].tolist()], maxlen=SVO_length, padding='post')\n",
    "verb_embeddings = pad_sequences([np.fromstring(embedding[1:-1], dtype=int, sep=',') for embedding in input_data[\"Verb\"].tolist()], maxlen=SVO_length, padding='post')\n",
    "object_embeddings = pad_sequences([np.fromstring(embedding[1:-1], dtype=int, sep=',') for embedding in input_data[\"Object\"].tolist()], maxlen=SVO_length, padding='post')\n",
    "\n",
    "# Adding all the embeddings together to truncate them when putting them in a model (instead of 128 * 3, it will be 128)\n",
    "svo_embeddings = subject_embeddings + verb_embeddings + object_embeddings\n",
    "\n",
    "# Shuffle and split the data\n",
    "c = list(zip(intensity_value, sentences, subject_embeddings, verb_embeddings, object_embeddings, svo_embeddings))\n",
    "random.shuffle(c)\n",
    "intensity_value, sentences, subject_embeddings, verb_embeddings, object_embeddings, svo_embeddings = zip(*c)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(sentences, intensity_value, test_size=TEST_SIZE, random_state=1)\n",
    "\n",
    "sentence_length = 128\n",
    "train_input_ids, train_input_masks, train_input_segment = tokenize(X_tr, subject_embeddings[:len(X_tr)], verb_embeddings[:len(X_tr)], object_embeddings[:len(X_tr)], svo_embeddings[:len(X_tr)], tokenizer, sentence_length)\n",
    "test_input_ids, test_input_masks, test_input_segment = tokenize(X_te, subject_embeddings[len(X_tr):], verb_embeddings[len(X_tr):], object_embeddings[len(X_tr):], svo_embeddings[len(X_tr):], tokenizer, sentence_length)\n",
    "\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16bdd356-0d72-46ae-a749-7181e27baa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input_ids[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e080f297-3122-4b7a-9dc7-8e26e08273ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(self.token_type_ids[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return input_ids, attention_mask, token_type_ids, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb717c35-c9c4-4b8f-bc23-f5c4e757c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_input_ids, train_input_masks, train_input_segment, y_tr)\n",
    "test_dataset = CustomDataset(test_input_ids, test_input_masks, test_input_segment, y_te)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bdc555b-4bcf-4273-a55f-8e3ca59ec30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, token_type_ids, targets in loader:\n",
    "        input_ids, attention_mask, token_type_ids, targets = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, token_type_ids, targets in loader:\n",
    "            input_ids, attention_mask, token_type_ids, targets = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), targets.to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de025d3-9a8a-4359-bd83-9ebfb151adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.embeddings.word_embeddings.weight: torch.Size([50265, 768])\n",
      "transformer.embeddings.position_embeddings.weight: torch.Size([514, 768])\n",
      "transformer.embeddings.token_type_embeddings.weight: torch.Size([1, 768])\n",
      "transformer.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "transformer.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "transformer.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "transformer.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "transformer.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "transformer.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "transformer.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "transformer.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "transformer.pooler.dense.weight: torch.Size([768, 768])\n",
      "transformer.pooler.dense.bias: torch.Size([768])\n",
      "lstm.weight_ih_l0: torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0: torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0: torch.Size([1024])\n",
      "lstm.bias_hh_l0: torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse: torch.Size([1024, 768])\n",
      "lstm.weight_hh_l0_reverse: torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse: torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse: torch.Size([1024])\n",
      "attention.in_proj_weight: torch.Size([1536, 512])\n",
      "attention.in_proj_bias: torch.Size([1536])\n",
      "attention.out_proj.weight: torch.Size([512, 512])\n",
      "attention.out_proj.bias: torch.Size([512])\n",
      "dense.weight: torch.Size([50, 512])\n",
      "dense.bias: torch.Size([50])\n",
      "output.weight: torch.Size([1, 50])\n",
      "output.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print PyTorch model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b1fbe2-9cc8-4361-98fa-21f62af24de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m mock_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tokenizer), size\u001b[38;5;241m=\u001b[39m(batch_size, max_length), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m mock_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(mock_input_ids)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mock_input_ids = torch.randint(low=0, high=len(tokenizer), size=(batch_size, max_length), dtype=torch.long).to(device)\n",
    "mock_attention_mask = torch.ones_like(mock_input_ids).to(device)\n",
    "\n",
    "summary(model.to(device), input_size=(batch_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d762dd23-04d1-4eb7-8c17-0a91b5528664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38bc7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 22\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     25\u001b[0m train_rmse \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(train_loss)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        targets = batch[2].to(device).float()  # Convert targets to float type\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Assuming outputs is of shape [batch_size, 1], reshape if necessary\n",
    "        outputs = outputs.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_rmse = math.sqrt(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            targets = batch[2].to(device).float()  # Convert targets to float type\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Assuming outputs is of shape [batch_size, 1], reshape if necessary\n",
    "            outputs = outputs.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "    val_loss = val_loss / len(test_loader.dataset)\n",
    "    val_rmse = math.sqrt(val_loss)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d51f6-5b1c-4b9d-8db8-1f35ee20103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model, test_loader, criterion, device)\n",
    "print(\"Test Loss:\", results)\n",
    "\n",
    "# Prediction\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, token_type_ids, labels in test_loader:\n",
    "        input_ids, attention_mask, token_type_ids, labels = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "predictions = np.array(predictions).flatten()\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "rmse = np.sqrt(np.mean((predictions - true_labels) ** 2))\n",
    "pearson = stats.pearsonr(predictions, true_labels)[0]\n",
    "cosine_sim = 1 - distance.cosine(predictions, true_labels)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Pearson:\", pearson)\n",
    "print(\"Cosine Similarity:\", cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4be075-5e68-4a15-9fc5-5d5f58ce8b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "research"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
