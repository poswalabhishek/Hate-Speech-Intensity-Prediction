{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8069b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the data from the CSV file\n",
    "csv_file_path = '../../datasets/rephrasal_of_sentences.csv'\n",
    "# Try reading the CSV file with a different encoding\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "\n",
    "# Define the base JSON structure\n",
    "json_data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\"You are a linguistic researcher specializing in evaluating the intensity of hate speech in sentences. \"\n",
    "                        \"Your task is to rate the intensity on a scale from 1 to 10, where 1 represents minimal hate speech and 10 represents extreme hate speech. \"\n",
    "                        \"This evaluation is crucial for creating a dataset that researchers can utilize to filter and understand harmful content effectively. \"\n",
    "                        \"Evaluate the hate intensity of the provided sentence. If the hate intensity exceeds 5, rephrase the sentence to reduce the intensity below 5 without altering its core message. \"\n",
    "                        \"Respond with a JSON object containing the sentence intensity, the rephrased sentence, and the new intensity.\")\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbb93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user and assistant messages to the JSON structure\n",
    "for _, row in df.iterrows():\n",
    "    json_data[\"messages\"].append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": row['Sentence']\n",
    "        }\n",
    "    )\n",
    "    json_data[\"messages\"].append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps({\n",
    "                \"intensity\": row['Original_Intensity'],\n",
    "                \"normalized_sentence\": row['Normalized_Sentence'],\n",
    "                \"normalized_intensity\": row['Normalized_Intensity']\n",
    "            })\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Write the JSON data to a file\n",
    "json_file_path = '../../datasets/json_datasets/rephrasal_of_sentences.json'\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05cf55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from helper import new_azure_credentials, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b82def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key= new_azure_credentials['api_key'],  \n",
    "    api_version= new_azure_credentials['api_version'],\n",
    "    azure_endpoint = new_azure_credentials['azure_endpoint']\n",
    "    )\n",
    "\n",
    "#This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. \n",
    "deployment_name='gpt-35-turbo' \n",
    "datasets_path = paths['datasets_path']\n",
    "json_datasets_path = paths['json_datasets_path']\n",
    "\n",
    "data = pd.read_csv(datasets_path + 'rephrasal_of_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e3204bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the training set into training and validation sets (80% of train_data for training, 20% for validation)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Function to convert DataFrame to JSONL format for GPT-3.5 Turbo\n",
    "def convert_to_json(data, file_path):\n",
    "    jsonl_data = []\n",
    "        \n",
    "    # Add user and assistant messages to the JSON structure\n",
    "    for _, row in data.iterrows():\n",
    "        try:\n",
    "            sentence = row['Sentence']\n",
    "            original_intensity = row['Original_Intensity']\n",
    "            normalized_sentence = row['Normalized_Sentence']\n",
    "            normalized_intensity = row['Normalized_Intensity']\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Available columns are: {data.columns}\")\n",
    "            return\n",
    "        \n",
    "        jsonl_data.append(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\"You are a linguistic researcher specializing in evaluating the intensity of hate speech in sentences. \"\n",
    "                                    \"Your task is to rate the intensity on a scale from 1 to 10, where 1 represents minimal hate speech and 10 represents extreme hate speech. \"\n",
    "                                    \"This evaluation is crucial for creating a dataset that researchers can utilize to filter and understand harmful content effectively. \"\n",
    "                                    \"Evaluate the hate intensity of the provided sentence. If the hate intensity exceeds 5, rephrase the sentence to reduce the intensity below 5 without altering its core message. \"\n",
    "                                    \"Respond with a JSON object containing the original intensity, the rephrased sentence, and the new intensity.\")\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": row['Sentence']\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": json.dumps({\n",
    "                            \"intensity\": row['Original_Intensity'],\n",
    "                            \"normalized_sentence\": row['Normalized_Sentence'],\n",
    "                            \"normalized_intensity\": row['Normalized_Intensity']\n",
    "                        })\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for entry in jsonl_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "convert_to_json(train_data, json_datasets_path + 'sentences/gpt_sentence_training_set.jsonl')\n",
    "convert_to_json(val_data, json_datasets_path + 'sentences/gpt_sentence_validation_set.jsonl')\n",
    "convert_to_json(test_data, json_datasets_path + 'sentences/gpt_sentence_test_set.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0068adbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Normalized_Sentence</th>\n",
       "      <th>Original_Intensity</th>\n",
       "      <th>Normalized_Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>How Important is Border Security to you ? Vote...</td>\n",
       "      <td>How Important is Border Security to you? Vote ...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>@user I hope that dumb cunt gets her face beat...</td>\n",
       "      <td>I hope that dumb woman gets her face beat in b...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>we need to change the priority of the refugee ...</td>\n",
       "      <td>we need to change the priority of the refugee ...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>i like how the jew is losing so its using a fa...</td>\n",
       "      <td>i like how the jew is losing so its using a fa...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>@user given that he some of the globalists top...</td>\n",
       "      <td>@user given that he some of the globalists top...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>honestly man most of you guys are nigger faggots</td>\n",
       "      <td>honestly man most of you guys are foul african...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>@user Listening to the world service this am c...</td>\n",
       "      <td>@J_VoiceUK Listening to the world service this...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>Another MS - 13 - linked body found after arre...</td>\n",
       "      <td>Another MS-13-linked body found after arrest o...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>god made the niggers he made them in the night...</td>\n",
       "      <td>god made the awful african american he made th...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>jew should be hated they are a disease that ne...</td>\n",
       "      <td>some jews should be hated they are a disease t...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1815 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "1568  How Important is Border Security to you ? Vote...   \n",
       "2021  @user I hope that dumb cunt gets her face beat...   \n",
       "541   we need to change the priority of the refugee ...   \n",
       "980   i like how the jew is losing so its using a fa...   \n",
       "1242  @user given that he some of the globalists top...   \n",
       "...                                                 ...   \n",
       "473    honestly man most of you guys are nigger faggots   \n",
       "1508  @user Listening to the world service this am c...   \n",
       "1812  Another MS - 13 - linked body found after arre...   \n",
       "451   god made the niggers he made them in the night...   \n",
       "421   jew should be hated they are a disease that ne...   \n",
       "\n",
       "                                    Normalized_Sentence  Original_Intensity  \\\n",
       "1568  How Important is Border Security to you? Vote ...                   8   \n",
       "2021  I hope that dumb woman gets her face beat in b...                   7   \n",
       "541   we need to change the priority of the refugee ...                   7   \n",
       "980   i like how the jew is losing so its using a fa...                   7   \n",
       "1242  @user given that he some of the globalists top...                   6   \n",
       "...                                                 ...                 ...   \n",
       "473   honestly man most of you guys are foul african...                   7   \n",
       "1508  @J_VoiceUK Listening to the world service this...                   7   \n",
       "1812  Another MS-13-linked body found after arrest o...                   8   \n",
       "451   god made the awful african american he made th...                   8   \n",
       "421   some jews should be hated they are a disease t...                   8   \n",
       "\n",
       "      Normalized_Intensity  \n",
       "1568                     5  \n",
       "2021                     6  \n",
       "541                      4  \n",
       "980                      3  \n",
       "1242                     2  \n",
       "...                    ...  \n",
       "473                      3  \n",
       "1508                     5  \n",
       "1812                     5  \n",
       "451                      3  \n",
       "421                      3  \n",
       "\n",
       "[1815 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c308c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-2f53f2cc54024ba5854351b9f8df261a\n",
      "Validation file ID: file-ed42f1b9f10c454cb7db5220989f40f2\n"
     ]
    }
   ],
   "source": [
    "training_file_name = json_datasets_path + 'sentences/gpt_sentence_training_set.jsonl'\n",
    "validation_file_name = json_datasets_path + 'sentences/gpt_sentence_validation_set.jsonl'\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "training_response = client.files.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cfe54ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-f5011e9b2fee459087339b41b96ee3ba\n",
      "Status: pending\n",
      "{\n",
      "  \"id\": \"ftjob-f5011e9b2fee459087339b41b96ee3ba\",\n",
      "  \"created_at\": 1719328037,\n",
      "  \"error\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": -1,\n",
      "    \"batch_size\": -1,\n",
      "    \"learning_rate_multiplier\": 1\n",
      "  },\n",
      "  \"model\": \"gpt-35-turbo-0125\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": null,\n",
      "  \"result_files\": null,\n",
      "  \"status\": \"pending\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-2f53f2cc54024ba5854351b9f8df261a\",\n",
      "  \"validation_file\": \"file-ed42f1b9f10c454cb7db5220989f40f2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-35-turbo-0125\",\n",
    "    # hypperparameters={}\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96e3f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-f5011e9b2fee459087339b41b96ee3ba\n",
      "Status: pending\n",
      "{\n",
      "  \"id\": \"ftjob-f5011e9b2fee459087339b41b96ee3ba\",\n",
      "  \"created_at\": 1719328037,\n",
      "  \"error\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"finished_at\": null,\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": -1,\n",
      "    \"batch_size\": -1,\n",
      "    \"learning_rate_multiplier\": 1\n",
      "  },\n",
      "  \"model\": \"gpt-35-turbo-0125\",\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"organization_id\": null,\n",
      "  \"result_files\": null,\n",
      "  \"status\": \"pending\",\n",
      "  \"trained_tokens\": null,\n",
      "  \"training_file\": \"file-2f53f2cc54024ba5854351b9f8df261a\",\n",
      "  \"validation_file\": \"file-ed42f1b9f10c454cb7db5220989f40f2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e183c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
